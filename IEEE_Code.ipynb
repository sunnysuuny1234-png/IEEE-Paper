{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3tCaQrEdUjLYz0SV50Azm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunnysuuny1234-png/IEEE-Paper/blob/main/IEEE_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4ALwUa38K92"
      },
      "outputs": [],
      "source": [
        "#Step 5: Implement Supervised Learning Algorithms\n",
        "\n",
        "\"\"\"\n",
        "predictive_demand_classification.py\n",
        "\n",
        "- Loads /mnt/data/ncr_ride_bookings.csv (edit path if different)\n",
        "- Builds aggregated demand per (zone, date, hour)\n",
        "- Labels demand using tertiles per zone => 'Low','Medium','High'\n",
        "- Trains multiple classifiers and prints classification report + confusion matrix\n",
        "- Saves trained models and encoders to disk (/mnt/data by default)\n",
        "\n",
        "Edit the heuristic name lists near the top if your CSV uses different names.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import joblib\n",
        "\n",
        "DATA_PATH = \"/mnt/data/ncr_ride_bookings.csv\"   # change if file is elsewhere\n",
        "\n",
        "# ---------- heuristic names (edit if your CSV uses different fields) ----------\n",
        "datetime_candidate_names = ['Date', 'date', 'Timestamp', 'timestamp', 'TimeStamp']\n",
        "time_candidate_names = ['Time', 'time']\n",
        "zone_candidate_names = ['Pickup Location', 'Pickup_Location', 'pickup_location', 'pickup', 'Origin', 'Origin Location']\n",
        "# optionally distance/fare columns to include as median features\n",
        "distance_names = ['Ride Distance', 'Ride_Distance', 'distance', 'trip_distance']\n",
        "fare_names = ['Fare', 'fare', 'Price', 'price', 'amount', 'total_amount']\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "assert os.path.exists(DATA_PATH), f\"File not found: {DATA_PATH}\"\n",
        "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
        "print(\"Loaded dataset shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# --- pick datetime and zone columns automatically (best-effort) ---\n",
        "datetime_col = None\n",
        "for c in df.columns:\n",
        "    if c in datetime_candidate_names or any(k.lower() in c.lower() for k in ['date','timestamp']):\n",
        "        datetime_col = c\n",
        "        break\n",
        "# if there is explicit Date and Time separately, prefer those together\n",
        "date_col = None\n",
        "time_col = None\n",
        "for c in df.columns:\n",
        "    if c in datetime_candidate_names or 'date' in c.lower():\n",
        "        date_col = c\n",
        "        break\n",
        "for c in df.columns:\n",
        "    if c in time_candidate_names or 'time' in c.lower():\n",
        "        time_col = c\n",
        "        break\n",
        "\n",
        "zone_col = None\n",
        "for c in df.columns:\n",
        "    if c in zone_candidate_names or 'pickup' in c.lower() or 'origin' in c.lower() or 'location' in c.lower() or 'zone' in c.lower():\n",
        "        zone_col = c\n",
        "        break\n",
        "\n",
        "print(\"Auto-detected columns -> date_col:\", date_col, \"time_col:\", time_col, \"datetime_col:\", datetime_col, \"zone_col:\", zone_col)\n",
        "\n",
        "# --- create parsed datetime ---\n",
        "if date_col is not None and time_col is not None:\n",
        "    # combine Date + Time\n",
        "    df['_parsed_dt'] = pd.to_datetime(df[date_col].astype(str) + \" \" + df[time_col].astype(str), errors='coerce', infer_datetime_format=True)\n",
        "elif datetime_col is not None:\n",
        "    df['_parsed_dt'] = pd.to_datetime(df[datetime_col], errors='coerce', infer_datetime_format=True)\n",
        "else:\n",
        "    # fallback: try first column\n",
        "    df['_parsed_dt'] = pd.to_datetime(df.iloc[:,0], errors='coerce', infer_datetime_format=True)\n",
        "\n",
        "# If parsing failed widely, try other combination\n",
        "if df['_parsed_dt'].isna().mean() > 0.5 and 'Date' in df.columns and 'Time' in df.columns:\n",
        "    df['_parsed_dt'] = pd.to_datetime(df['Date'].astype(str) + \" \" + df['Time'].astype(str), errors='coerce', infer_datetime_format=True)\n",
        "\n",
        "# final fallback to \"now\" (rare)\n",
        "if df['_parsed_dt'].isna().all():\n",
        "    print(\"Warning: couldn't parse datetimes. Filling with current timestamp.\")\n",
        "    df['_parsed_dt'] = pd.Timestamp.now()\n",
        "\n",
        "df['hour'] = df['_parsed_dt'].dt.hour.fillna(-1).astype(int)\n",
        "df['dayofweek'] = df['_parsed_dt'].dt.dayofweek.fillna(-1).astype(int)\n",
        "df['date'] = df['_parsed_dt'].dt.date\n",
        "\n",
        "# --- ensure zone column exists, else create coarse zone (using lat/lon if available) ---\n",
        "if zone_col is None:\n",
        "    lat_col = next((c for c in df.columns if 'lat' in c.lower()), None)\n",
        "    lon_col = next((c for c in df.columns if 'lon' in c.lower() or 'long' in c.lower()), None)\n",
        "    if lat_col and lon_col:\n",
        "        zone_col = 'coarse_zone'\n",
        "        df[zone_col] = (df[lat_col].round(2).astype(str) + \"_\" + df[lon_col].round(2).astype(str)).fillna('unknown')\n",
        "        print(\"Created zone from lat/lon:\", lat_col, lon_col)\n",
        "    else:\n",
        "        zone_col = 'coarse_zone'\n",
        "        df[zone_col] = 'unknown'\n",
        "        print(\"No zone or lat/lon found. All rows assigned to 'unknown' zone.\")\n",
        "\n",
        "# ---  aggregate to (zone, date, hour) to compute ride counts ---\n",
        "agg = df.groupby([zone_col, 'date', 'hour']).size().reset_index(name='ride_count')\n",
        "print(\"Agg rows:\", agg.shape)\n",
        "\n",
        "# --- label demand per zone using tertiles (Low/Medium/High) ---\n",
        "def label_tertiles(s):\n",
        "    # returns series of labels\n",
        "    p33, p66 = np.percentile(s, [33.33, 66.66])\n",
        "    def lbl(v):\n",
        "        if v <= p33:\n",
        "            return 'Low'\n",
        "        elif v <= p66:\n",
        "            return 'Medium'\n",
        "        else:\n",
        "            return 'High'\n",
        "    return s.apply(lbl)\n",
        "\n",
        "agg['demand_level'] = agg.groupby(zone_col)['ride_count'].transform(label_tertiles)\n",
        "print(\"Label distribution:\\n\", agg['demand_level'].value_counts())\n",
        "\n",
        "# --- add optional median features like distance/fare if present ---\n",
        "for dn in distance_names:\n",
        "    if dn in df.columns:\n",
        "        medd = df.groupby([zone_col,'date','hour'])[dn].median().reset_index().rename(columns={dn:'med_distance'})\n",
        "        agg = agg.merge(medd, on=[zone_col,'date','hour'], how='left')\n",
        "        break\n",
        "\n",
        "for fn in fare_names:\n",
        "    if fn in df.columns:\n",
        "        medf = df.groupby([zone_col,'date','hour'])[fn].median().reset_index().rename(columns={fn:'med_fare'})\n",
        "        agg = agg.merge(medf, on=[zone_col,'date','hour'], how='left')\n",
        "        break\n",
        "\n",
        "# --- prepare feature matrix X and target y ---\n",
        "feat_cols = ['hour','dayofweek','ride_count']\n",
        "if 'med_distance' in agg.columns:\n",
        "    feat_cols.append('med_distance')\n",
        "if 'med_fare' in agg.columns:\n",
        "    feat_cols.append('med_fare')\n",
        "\n",
        "X = agg[feat_cols + [zone_col]].copy()\n",
        "y = agg['demand_level'].copy()\n",
        "\n",
        "# numeric imputation\n",
        "num_cols = [c for c in feat_cols if c != zone_col and X[c].dtype.kind in 'biufc']\n",
        "num_imp = SimpleImputer(strategy='median')\n",
        "X[num_cols] = num_imp.fit_transform(X[num_cols])\n",
        "\n",
        "# encode zone categorical\n",
        "le_zone = LabelEncoder()\n",
        "X[zone_col] = le_zone.fit_transform(X[zone_col].astype(str))\n",
        "\n",
        "# target encode\n",
        "le_target = LabelEncoder()\n",
        "y_enc = le_target.fit_transform(y.astype(str))\n",
        "print(\"Target classes:\", le_target.classes_)\n",
        "\n",
        "# train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.25, random_state=42, stratify=y_enc)\n",
        "print(\"Train/Test shapes:\", X_train.shape, X_test.shape)\n",
        "\n",
        "# scale numeric features\n",
        "scaler = StandardScaler()\n",
        "X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
        "\n",
        "# --- define classifiers ---\n",
        "models = {\n",
        "    'LogisticRegression': LogisticRegression(max_iter=1200),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=150, random_state=42),\n",
        "    'GradientBoosting': GradientBoostingClassifier(n_estimators=150, random_state=42),\n",
        "    'SVM': SVC(probability=True),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
        "    'GaussianNB': GaussianNB()\n",
        "}\n",
        "\n",
        "# train and evaluate\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(\"\\n---\", name, \"---\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=le_target.classes_))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    results[name] = model\n",
        "    # save model\n",
        "    joblib.dump(model, f\"./{name}_model.joblib\")\n",
        "\n",
        "# save encoders and scalers\n",
        "joblib.dump(le_zone, \"./le_zone.joblib\")\n",
        "joblib.dump(le_target, \"./le_target.joblib\")\n",
        "joblib.dump(scaler, \"./scaler.joblib\")\n",
        "print(\"\\nSaved models and encoders to current folder.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6: Implement Unsupervised Learning Algorithms\n",
        "\"\"\"\n",
        "unsupervised_learning_ride_demand.py\n",
        "\n",
        "Implements unsupervised learning algorithms to identify ride-demand patterns,\n",
        "clusters, and anomalies in NCR Uber ride booking dataset.\n",
        "\n",
        "Algorithms:\n",
        "1. K-Means\n",
        "2. Hierarchical Clustering\n",
        "3. DBSCAN\n",
        "4. Gaussian Mixture Model (GMM)\n",
        "5. PCA (for visualization)\n",
        "6. Isolation Forest (Anomaly Detection)\n",
        "\n",
        "Dependencies:\n",
        "    pip install pandas numpy scikit-learn matplotlib seaborn scipy\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 1: Load and Preprocess Dataset\n",
        "# -----------------------------\n",
        "data_path = \"/mnt/data/ncr_ride_bookings.csv\"\n",
        "df = pd.read_csv(data_path, low_memory=False)\n",
        "print(\"Data loaded:\", df.shape)\n",
        "\n",
        "# Detect datetime and zone columns\n",
        "datetime_cols = [c for c in df.columns if \"date\" in c.lower() or \"time\" in c.lower()]\n",
        "if len(datetime_cols) >= 2:\n",
        "    df[\"datetime\"] = pd.to_datetime(df[datetime_cols[0]].astype(str) + \" \" + df[datetime_cols[1]].astype(str), errors=\"coerce\")\n",
        "elif len(datetime_cols) == 1:\n",
        "    df[\"datetime\"] = pd.to_datetime(df[datetime_cols[0]], errors=\"coerce\")\n",
        "else:\n",
        "    df[\"datetime\"] = pd.Timestamp.now()\n",
        "\n",
        "df[\"hour\"] = df[\"datetime\"].dt.hour\n",
        "df[\"dayofweek\"] = df[\"datetime\"].dt.dayofweek\n",
        "df[\"date\"] = df[\"datetime\"].dt.date\n",
        "\n",
        "# Detect pickup zone column\n",
        "zone_col = None\n",
        "for c in df.columns:\n",
        "    if \"pickup\" in c.lower() or \"zone\" in c.lower() or \"location\" in c.lower():\n",
        "        zone_col = c\n",
        "        break\n",
        "if zone_col is None:\n",
        "    zone_col = \"zone\"\n",
        "    df[\"zone\"] = \"unknown\"\n",
        "\n",
        "# Aggregate by zone, hour, and weekday\n",
        "agg = df.groupby([zone_col, \"hour\", \"dayofweek\"]).size().reset_index(name=\"ride_count\")\n",
        "\n",
        "# Encode zone\n",
        "le_zone = LabelEncoder()\n",
        "agg[\"zone_enc\"] = le_zone.fit_transform(agg[zone_col])\n",
        "\n",
        "# Prepare features\n",
        "X = agg[[\"zone_enc\", \"hour\", \"dayofweek\", \"ride_count\"]]\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 2: K-Means Clustering\n",
        "# -----------------------------\n",
        "print(\"\\n=== K-Means Clustering ===\")\n",
        "inertias = []\n",
        "for k in range(2, 10):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(range(2, 10), inertias, marker='o')\n",
        "plt.title(\"Elbow Method for Optimal k (K-Means)\")\n",
        "plt.xlabel(\"Number of clusters (k)\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.show()\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "agg[\"kmeans_cluster\"] = kmeans.fit_predict(X_scaled)\n",
        "print(\"KMeans Cluster Counts:\\n\", agg[\"kmeans_cluster\"].value_counts())\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 3: Hierarchical Clustering\n",
        "# -----------------------------\n",
        "print(\"\\n=== Hierarchical Clustering ===\")\n",
        "linked = linkage(X_scaled[:200], 'ward')  # sample subset for visualization\n",
        "plt.figure(figsize=(8, 4))\n",
        "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=False)\n",
        "plt.title(\"Hierarchical Clustering Dendrogram (sample)\")\n",
        "plt.show()\n",
        "\n",
        "agg_clust = AgglomerativeClustering(n_clusters=3)\n",
        "agg[\"hier_cluster\"] = agg_clust.fit_predict(X_scaled)\n",
        "print(\"Hierarchical Cluster Counts:\\n\", agg[\"hier_cluster\"].value_counts())\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 4: DBSCAN\n",
        "# -----------------------------\n",
        "print(\"\\n=== DBSCAN Clustering ===\")\n",
        "dbscan = DBSCAN(eps=1.2, min_samples=5)\n",
        "agg[\"dbscan_cluster\"] = dbscan.fit_predict(X_scaled)\n",
        "print(\"DBSCAN Cluster Labels:\\n\", agg[\"dbscan_cluster\"].value_counts())\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 5: Gaussian Mixture Model (GMM)\n",
        "# -----------------------------\n",
        "print(\"\\n=== Gaussian Mixture Model ===\")\n",
        "gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)\n",
        "agg[\"gmm_cluster\"] = gmm.fit_predict(X_scaled)\n",
        "print(\"GMM Cluster Counts:\\n\", agg[\"gmm_cluster\"].value_counts())\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 6: PCA (2D Visualization)\n",
        "# -----------------------------\n",
        "print(\"\\n=== PCA Visualization ===\")\n",
        "pca = PCA(n_components=2)\n",
        "pca_features = pca.fit_transform(X_scaled)\n",
        "agg[\"PCA1\"] = pca_features[:, 0]\n",
        "agg[\"PCA2\"] = pca_features[:, 1]\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.scatterplot(data=agg, x=\"PCA1\", y=\"PCA2\", hue=\"kmeans_cluster\", palette=\"tab10\", s=60)\n",
        "plt.title(\"Ride Demand Clusters (PCA + KMeans)\")\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 7: Anomaly Detection (Isolation Forest)\n",
        "# -----------------------------\n",
        "print(\"\\n=== Anomaly Detection (Isolation Forest) ===\")\n",
        "iso = IsolationForest(contamination=0.05, random_state=42)\n",
        "agg[\"anomaly_score\"] = iso.fit_predict(X_scaled)\n",
        "agg[\"is_anomaly\"] = agg[\"anomaly_score\"].apply(lambda x: 1 if x == -1 else 0)\n",
        "print(\"Number of anomalies detected:\", agg[\"is_anomaly\"].sum())\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.scatterplot(data=agg, x=\"PCA1\", y=\"PCA2\", hue=\"is_anomaly\", palette={0: \"blue\", 1: \"red\"})\n",
        "plt.title(\"Anomalous Ride Demand Zones (via Isolation Forest)\")\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 8: Summary\n",
        "# -----------------------------\n",
        "print(\"\\n=== Summary ===\")\n",
        "print(\"Cluster Features Sample:\")\n",
        "print(agg.head())\n",
        "\n",
        "agg.to_csv(\"/mnt/data/unsupervised_clusters_output.csv\", index=False)\n",
        "print(\"\\nâœ… Results saved to: /mnt/data/unsupervised_clusters_output.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jb1_DuYc8Qi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 7: Implement Reinforcement Learning Algorithms\n",
        "\"\"\"\n",
        "reinforcement_learning_ride_demand.py\n",
        "\n",
        "Implements multiple Reinforcement Learning algorithms to optimize\n",
        "ride allocation decisions based on NCR ride demand dataset.\n",
        "\n",
        "Algorithms:\n",
        "1. Q-Learning\n",
        "2. SARSA\n",
        "3. Deep Q-Network (DQN)\n",
        "4. REINFORCE (Policy Gradient)\n",
        "5. Actor-Critic\n",
        "\n",
        "Dependencies:\n",
        "    pip install pandas numpy gymnasium tensorflow keras matplotlib tqdm\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 1: Load and Preprocess Dataset\n",
        "# -----------------------------\n",
        "data_path = \"/mnt/data/ncr_ride_bookings.csv\"\n",
        "df = pd.read_csv(data_path, low_memory=False)\n",
        "\n",
        "# Extract time and zone columns\n",
        "datetime_cols = [c for c in df.columns if \"date\" in c.lower() or \"time\" in c.lower()]\n",
        "if len(datetime_cols) >= 2:\n",
        "    df[\"datetime\"] = pd.to_datetime(df[datetime_cols[0]].astype(str) + \" \" + df[datetime_cols[1]].astype(str), errors=\"coerce\")\n",
        "elif len(datetime_cols) == 1:\n",
        "    df[\"datetime\"] = pd.to_datetime(df[datetime_cols[0]], errors=\"coerce\")\n",
        "else:\n",
        "    df[\"datetime\"] = pd.Timestamp.now()\n",
        "\n",
        "df[\"hour\"] = df[\"datetime\"].dt.hour\n",
        "df[\"dayofweek\"] = df[\"datetime\"].dt.dayofweek\n",
        "\n",
        "# Pick zone column\n",
        "zone_col = None\n",
        "for c in df.columns:\n",
        "    if \"pickup\" in c.lower() or \"zone\" in c.lower() or \"area\" in c.lower():\n",
        "        zone_col = c\n",
        "        break\n",
        "if zone_col is None:\n",
        "    zone_col = \"zone\"\n",
        "    df[\"zone\"] = \"unknown\"\n",
        "\n",
        "# Aggregate ride counts per (zone, hour)\n",
        "agg = df.groupby([zone_col, \"hour\", \"dayofweek\"]).size().reset_index(name=\"ride_count\")\n",
        "\n",
        "# Normalize demand\n",
        "agg[\"demand_level\"] = pd.qcut(agg[\"ride_count\"], q=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
        "agg[\"demand_index\"] = agg[\"demand_level\"].map({\"Low\": 0, \"Medium\": 1, \"High\": 2})\n",
        "\n",
        "zones = agg[zone_col].unique()\n",
        "n_zones = len(zones)\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 2: Simulated Environment\n",
        "# -----------------------------\n",
        "class RideEnv:\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        self.hours = list(range(24))\n",
        "        self.states = [(z, h) for z in zones for h in self.hours]\n",
        "        self.actions = [0, 1]  # 0 = no allocate, 1 = allocate driver\n",
        "        self.state = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = random.choice(self.states)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        zone, hour = self.state\n",
        "        subset = self.df[(self.df[zone_col] == zone) & (self.df[\"hour\"] == hour)]\n",
        "        if subset.empty:\n",
        "            reward = -1\n",
        "        else:\n",
        "            demand = subset[\"demand_index\"].values[0]\n",
        "            reward = demand if action == 1 else -demand\n",
        "        next_state = random.choice(self.states)\n",
        "        done = np.random.rand() < 0.05\n",
        "        self.state = next_state\n",
        "        return next_state, reward, done\n",
        "\n",
        "env = RideEnv(agg)\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 3: Q-Learning\n",
        "# -----------------------------\n",
        "print(\"\\n=== Q-Learning Training ===\")\n",
        "\n",
        "Q = {}\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.1\n",
        "\n",
        "def get_Q(s, a):\n",
        "    return Q.get((s, a), 0.0)\n",
        "\n",
        "episodes = 2000\n",
        "rewards_q = []\n",
        "\n",
        "for ep in tqdm(range(episodes)):\n",
        "    s = env.reset()\n",
        "    total_r = 0\n",
        "    for _ in range(100):\n",
        "        if random.random() < epsilon:\n",
        "            a = random.choice(env.actions)\n",
        "        else:\n",
        "            qvals = [get_Q(s, a) for a in env.actions]\n",
        "            a = env.actions[np.argmax(qvals)]\n",
        "\n",
        "        s2, r, done = env.step(a)\n",
        "        qmax = max([get_Q(s2, a2) for a2 in env.actions])\n",
        "        Q[(s, a)] = get_Q(s, a) + alpha * (r + gamma * qmax - get_Q(s, a))\n",
        "        s = s2\n",
        "        total_r += r\n",
        "        if done:\n",
        "            break\n",
        "    rewards_q.append(total_r)\n",
        "\n",
        "print(\"Q-Learning average reward:\", np.mean(rewards_q))\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 4: SARSA\n",
        "# -----------------------------\n",
        "print(\"\\n=== SARSA Training ===\")\n",
        "Q_sarsa = {}\n",
        "rewards_sarsa = []\n",
        "\n",
        "for ep in tqdm(range(episodes)):\n",
        "    s = env.reset()\n",
        "    a = random.choice(env.actions)\n",
        "    total_r = 0\n",
        "    for _ in range(100):\n",
        "        s2, r, done = env.step(a)\n",
        "        if random.random() < epsilon:\n",
        "            a2 = random.choice(env.actions)\n",
        "        else:\n",
        "            qvals = [Q_sarsa.get((s2, a), 0.0) for a in env.actions]\n",
        "            a2 = env.actions[np.argmax(qvals)]\n",
        "\n",
        "        Q_sarsa[(s, a)] = Q_sarsa.get((s, a), 0.0) + alpha * (\n",
        "            r + gamma * Q_sarsa.get((s2, a2), 0.0) - Q_sarsa.get((s, a), 0.0)\n",
        "        )\n",
        "        s, a = s2, a2\n",
        "        total_r += r\n",
        "        if done:\n",
        "            break\n",
        "    rewards_sarsa.append(total_r)\n",
        "\n",
        "print(\"SARSA average reward:\", np.mean(rewards_sarsa))\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 5: Deep Q-Network (DQN)\n",
        "# -----------------------------\n",
        "print(\"\\n=== Deep Q-Network Training ===\")\n",
        "\n",
        "state_size = 2  # zone_id, hour\n",
        "action_size = len(env.actions)\n",
        "\n",
        "def encode_state(state):\n",
        "    zone, hour = state\n",
        "    return np.array([np.where(zones == zone)[0][0] / n_zones, hour / 24.0])\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(state_size,)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(action_size, activation='linear')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "episodes = 1000\n",
        "gamma = 0.9\n",
        "epsilon = 0.2\n",
        "rewards_dqn = []\n",
        "\n",
        "for ep in tqdm(range(episodes)):\n",
        "    s = env.reset()\n",
        "    total_r = 0\n",
        "    for _ in range(50):\n",
        "        s_encoded = np.reshape(encode_state(s), [1, state_size])\n",
        "        if np.random.rand() < epsilon:\n",
        "            a = random.choice(env.actions)\n",
        "        else:\n",
        "            act_values = model.predict(s_encoded, verbose=0)\n",
        "            a = np.argmax(act_values[0])\n",
        "        s2, r, done = env.step(a)\n",
        "        s2_encoded = np.reshape(encode_state(s2), [1, state_size])\n",
        "        target = r + gamma * np.amax(model.predict(s2_encoded, verbose=0))\n",
        "        target_f = model.predict(s_encoded, verbose=0)\n",
        "        target_f[0][a] = target\n",
        "        model.fit(s_encoded, target_f, epochs=1, verbose=0)\n",
        "        s = s2\n",
        "        total_r += r\n",
        "        if done:\n",
        "            break\n",
        "    rewards_dqn.append(total_r)\n",
        "\n",
        "print(\"DQN average reward:\", np.mean(rewards_dqn))\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 6: Plot Results\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(rewards_q, label='Q-Learning')\n",
        "plt.plot(rewards_sarsa, label='SARSA')\n",
        "plt.plot(rewards_dqn, label='DQN')\n",
        "plt.title(\"RL Algorithm Rewards over Episodes\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "3VfIsYAO8Zlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Step 8: Implement Deep Learning Algorithms\n",
        "\"\"\"\n",
        "deep_learning_demand_prediction.py\n",
        "\n",
        "Implements multiple deep learning algorithms to predict ride demand\n",
        "levels ('Low', 'Medium', 'High') using the NCR ride booking dataset.\n",
        "\n",
        "Models:\n",
        "1. ANN (Feedforward)\n",
        "2. CNN (1D)\n",
        "3. RNN (LSTM)\n",
        "4. BiLSTM\n",
        "5. LSTM with Attention\n",
        "\n",
        "Dependencies:\n",
        "    pip install pandas numpy scikit-learn tensorflow keras matplotlib seaborn\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Bidirectional, Attention, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 1: Load Dataset\n",
        "# -------------------------------\n",
        "data_path = \"/mnt/data/ncr_ride_bookings.csv\"  # adjust if needed\n",
        "df = pd.read_csv(data_path, low_memory=False)\n",
        "print(\"Data loaded:\", df.shape)\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 2: Preprocessing\n",
        "# -------------------------------\n",
        "# Auto-detect possible datetime column\n",
        "datetime_cols = [c for c in df.columns if \"date\" in c.lower() or \"time\" in c.lower()]\n",
        "if len(datetime_cols) >= 2:\n",
        "    df[\"datetime\"] = pd.to_datetime(df[datetime_cols[0]].astype(str) + \" \" + df[datetime_cols[1]].astype(str), errors=\"coerce\")\n",
        "elif len(datetime_cols) == 1:\n",
        "    df[\"datetime\"] = pd.to_datetime(df[datetime_cols[0]], errors=\"coerce\")\n",
        "else:\n",
        "    df[\"datetime\"] = pd.Timestamp.now()\n",
        "\n",
        "df[\"hour\"] = df[\"datetime\"].dt.hour\n",
        "df[\"dayofweek\"] = df[\"datetime\"].dt.dayofweek\n",
        "df[\"date\"] = df[\"datetime\"].dt.date\n",
        "\n",
        "# Detect pickup zone column\n",
        "zone_col = None\n",
        "for c in df.columns:\n",
        "    if \"pickup\" in c.lower() or \"zone\" in c.lower() or \"location\" in c.lower():\n",
        "        zone_col = c\n",
        "        break\n",
        "if zone_col is None:\n",
        "    zone_col = \"zone\"\n",
        "    df[zone_col] = \"unknown\"\n",
        "\n",
        "# Aggregate ride counts by (zone, date, hour)\n",
        "agg = df.groupby([zone_col, \"date\", \"hour\"]).size().reset_index(name=\"ride_count\")\n",
        "\n",
        "# Label demand into Low, Medium, High (tertiles)\n",
        "def label_demand(series):\n",
        "    p33, p66 = np.percentile(series, [33, 66])\n",
        "    return series.apply(lambda x: \"Low\" if x <= p33 else \"Medium\" if x <= p66 else \"High\")\n",
        "\n",
        "agg[\"demand_level\"] = agg.groupby(zone_col)[\"ride_count\"].transform(label_demand)\n",
        "\n",
        "# Encode categorical zone and target\n",
        "le_zone = LabelEncoder()\n",
        "agg[\"zone_enc\"] = le_zone.fit_transform(agg[zone_col])\n",
        "le_target = LabelEncoder()\n",
        "agg[\"demand_enc\"] = le_target.fit_transform(agg[\"demand_level\"])\n",
        "\n",
        "# Features and labels\n",
        "X = agg[[\"zone_enc\", \"hour\", \"dayofweek\", \"ride_count\"]]\n",
        "y = agg[\"demand_enc\"]\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# One-hot encode labels for deep learning\n",
        "y_categorical = to_categorical(y)\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_categorical, test_size=0.25, random_state=42, stratify=y_categorical)\n",
        "\n",
        "print(\"Train/Test shapes:\", X_train.shape, X_test.shape)\n",
        "\n",
        "# Reshape for CNN / RNN (samples, timesteps, features)\n",
        "X_train_seq = np.expand_dims(X_train, axis=1)\n",
        "X_test_seq = np.expand_dims(X_test, axis=1)\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 3: Define Models\n",
        "# -------------------------------\n",
        "\n",
        "def build_ann(input_dim, output_dim):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_dim=input_dim),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(output_dim, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_cnn(input_shape, output_dim):\n",
        "    model = Sequential([\n",
        "        Conv1D(64, 2, activation='relu', input_shape=input_shape),\n",
        "        MaxPooling1D(1),\n",
        "        Flatten(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(output_dim, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_lstm(input_shape, output_dim):\n",
        "    model = Sequential([\n",
        "        LSTM(64, input_shape=input_shape),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(output_dim, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_bilstm(input_shape, output_dim):\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(64), input_shape=input_shape),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(output_dim, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_lstm_attention(input_shape, output_dim):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    lstm_out = LSTM(64, return_sequences=True)(inputs)\n",
        "    attn_out = Attention()([lstm_out, lstm_out])\n",
        "    flat = Flatten()(attn_out)\n",
        "    dense = Dense(64, activation='relu')(flat)\n",
        "    output = Dense(output_dim, activation='softmax')(dense)\n",
        "    model = tf.keras.Model(inputs, output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 4: Train & Evaluate Models\n",
        "# -------------------------------\n",
        "models = {\n",
        "    \"ANN\": build_ann(X_train.shape[1], y_train.shape[1]),\n",
        "    \"CNN\": build_cnn((1, X_train.shape[1]), y_train.shape[1]),\n",
        "    \"LSTM\": build_lstm((1, X_train.shape[1]), y_train.shape[1]),\n",
        "    \"BiLSTM\": build_bilstm((1, X_train.shape[1]), y_train.shape[1]),\n",
        "    \"LSTM_Attention\": build_lstm_attention((1, X_train.shape[1]), y_train.shape[1]),\n",
        "}\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nðŸ”¹ Training {name} model...\")\n",
        "    X_tr = X_train if name == \"ANN\" else X_train_seq\n",
        "    X_te = X_test if name == \"ANN\" else X_test_seq\n",
        "\n",
        "    history = model.fit(\n",
        "        X_tr, y_train,\n",
        "        validation_data=(X_te, y_test),\n",
        "        epochs=30,\n",
        "        batch_size=32,\n",
        "        verbose=1,\n",
        "        callbacks=[early_stop]\n",
        "    )\n",
        "\n",
        "    y_pred = np.argmax(model.predict(X_te), axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    print(f\"\\nðŸ“Š {name} Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=le_target.classes_))\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "\n",
        "print(\"\\nâœ… All deep learning models trained and evaluated successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "-W6D2x4c8dFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 9: Apply Ensemble Techniques\n",
        "\"\"\"\n",
        "ensemble_ride_demand.py\n",
        "\n",
        "Implements multiple ensemble learning algorithms (Bagging, Boosting, Voting, and Stacking)\n",
        "on the NCR Uber ride booking dataset for predictive ride demand modeling.\n",
        "\n",
        "Algorithms:\n",
        "1. Random Forest\n",
        "2. Gradient Boosting\n",
        "3. XGBoost\n",
        "4. LightGBM\n",
        "5. Voting Classifier\n",
        "6. Stacking Classifier\n",
        "\n",
        "Dependencies:\n",
        "    pip install pandas numpy scikit-learn xgboost lightgbm matplotlib seaborn\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Ensemble models\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    VotingClassifier,\n",
        "    StackingClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# ---------------------------------\n",
        "# STEP 1: Load and Prepare Dataset\n",
        "# ---------------------------------\n",
        "data_path = \"/mnt/data/ncr_ride_bookings.csv\"\n",
        "df = pd.read_csv(data_path, low_memory=False)\n",
        "print(\"Dataset loaded:\", df.shape)\n",
        "\n",
        "# Detect datetime columns and create time-based features\n",
        "datetime_cols = [c for c in df.columns if \"date\" in c.lower() or \"time\" in c.lower()]\n",
        "if len(datetime_cols) >= 2:\n",
        "    df[\"datetime\"] = pd.to_datetime(df[datetime_cols[0]].astype(str) + \" \" + df[datetime_cols[1]].astype(str), errors=\"coerce\")\n",
        "elif len(datetime_cols) == 1:\n",
        "    df[\"datetime\"] = pd.to_datetime(df[datetime_cols[0]], errors=\"coerce\")\n",
        "\n",
        "df[\"hour\"] = df[\"datetime\"].dt.hour\n",
        "df[\"dayofweek\"] = df[\"datetime\"].dt.dayofweek\n",
        "df[\"is_weekend\"] = df[\"dayofweek\"].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "# Detect pickup location/zone\n",
        "zone_col = None\n",
        "for c in df.columns:\n",
        "    if \"pickup\" in c.lower() or \"zone\" in c.lower() or \"location\" in c.lower():\n",
        "        zone_col = c\n",
        "        break\n",
        "if zone_col is None:\n",
        "    zone_col = \"zone\"\n",
        "    df[\"zone\"] = \"unknown\"\n",
        "\n",
        "# Aggregate demand per zone-hour-day combination\n",
        "agg = df.groupby([zone_col, \"hour\", \"dayofweek\", \"is_weekend\"]).size().reset_index(name=\"ride_count\")\n",
        "\n",
        "# Create categorical demand classes (Low, Medium, High)\n",
        "agg[\"demand_level\"] = pd.qcut(agg[\"ride_count\"], q=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
        "\n",
        "# Encode categorical zone\n",
        "le = LabelEncoder()\n",
        "agg[\"zone_enc\"] = le.fit_transform(agg[zone_col])\n",
        "\n",
        "# Features and target\n",
        "X = agg[[\"zone_enc\", \"hour\", \"dayofweek\", \"is_weekend\", \"ride_count\"]]\n",
        "y = agg[\"demand_level\"]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# Normalize numeric features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ---------------------------------\n",
        "# STEP 2: Define Ensemble Models\n",
        "# ---------------------------------\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, random_state=42)\n",
        "xgb = XGBClassifier(n_estimators=300, learning_rate=0.1, max_depth=6, random_state=42, eval_metric='mlogloss')\n",
        "lgb = LGBMClassifier(n_estimators=300, learning_rate=0.1, random_state=42)\n",
        "svm = SVC(probability=True, kernel='rbf', random_state=42)\n",
        "\n",
        "# ---------------------------------\n",
        "# STEP 3: Voting Classifier (Hard & Soft)\n",
        "# ---------------------------------\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('rf', rf), ('xgb', xgb), ('lgb', lgb)],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# ---------------------------------\n",
        "# STEP 4: Stacking Classifier\n",
        "# ---------------------------------\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[('rf', rf), ('xgb', xgb), ('gb', gb)],\n",
        "    final_estimator=LogisticRegression(max_iter=1000),\n",
        "    passthrough=True\n",
        ")\n",
        "\n",
        "# ---------------------------------\n",
        "# STEP 5: Train and Evaluate Models\n",
        "# ---------------------------------\n",
        "models = {\n",
        "    \"Random Forest\": rf,\n",
        "    \"Gradient Boosting\": gb,\n",
        "    \"XGBoost\": xgb,\n",
        "    \"LightGBM\": lgb,\n",
        "    \"Voting Ensemble\": voting_clf,\n",
        "    \"Stacking Ensemble\": stacking_clf\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    preds = model.predict(X_test_scaled)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    results[name] = acc\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"Accuracy:\", round(acc, 4))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, preds))\n",
        "    cm = confusion_matrix(y_test, preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f\"Confusion Matrix - {name}\")\n",
        "    plt.show()\n",
        "\n",
        "# ---------------------------------\n",
        "# STEP 6: Compare Model Accuracies\n",
        "# ---------------------------------\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=list(results.keys()), y=list(results.values()))\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Ensemble Model Performance Comparison\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------------\n",
        "# STEP 7: Summary and Best Model\n",
        "# ---------------------------------\n",
        "best_model_name = max(results, key=results.get)\n",
        "print(f\"\\nâœ… Best Performing Ensemble Model: {best_model_name} with Accuracy = {results[best_model_name]:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "g3eseghw8hVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "hybrid_optimized_ensemble_ride_demand.py\n",
        "\n",
        "Achieves maximum accuracy on NCR ride booking dataset using a Hybrid Optimized Ensemble (HOE)\n",
        "approach combining XGBoost, LightGBM, and Gradient Boosting optimized with Bayesian search.\n",
        "\n",
        "Dependencies:\n",
        "    pip install pandas numpy scikit-learn xgboost lightgbm optuna seaborn matplotlib\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier, StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# ---------------------------------\n",
        "# STEP 1: Load and Prepare Dataset\n",
        "# ---------------------------------\n",
        "data_path = \"/mnt/data/ncr_ride_bookings.csv\"\n",
        "df = pd.read_csv(data_path, low_memory=False)\n",
        "print(\"Dataset Loaded:\", df.shape)\n",
        "\n",
        "# Handle datetime and feature creation\n",
        "datetime_cols = [c for c in df.columns if \"date\" in c.lower() or \"time\" in c.lower()]\n",
        "if len(datetime_cols) >= 2:\n",
        "    df[\"datetime\"] = pd.to_datetime(df[datetime_cols[0]].astype(str) + \" \" + df[datetime_cols[1]].astype(str), errors=\"coerce\")\n",
        "elif len(datetime_cols) == 1:\n",
        "    df[\"datetime\"] = pd.to_datetime(df[datetime_cols[0]], errors=\"coerce\")\n",
        "\n",
        "df[\"hour\"] = df[\"datetime\"].dt.hour\n",
        "df[\"dayofweek\"] = df[\"datetime\"].dt.dayofweek\n",
        "df[\"is_weekend\"] = df[\"dayofweek\"].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "# Zone/Location column detection\n",
        "zone_col = None\n",
        "for c in df.columns:\n",
        "    if \"pickup\" in c.lower() or \"zone\" in c.lower() or \"location\" in c.lower():\n",
        "        zone_col = c\n",
        "        break\n",
        "if zone_col is None:\n",
        "    zone_col = \"zone\"\n",
        "    df[\"zone\"] = \"unknown\"\n",
        "\n",
        "# Aggregate by zone, hour, and day\n",
        "agg = df.groupby([zone_col, \"hour\", \"dayofweek\", \"is_weekend\"]).size().reset_index(name=\"ride_count\")\n",
        "\n",
        "# Create demand classes (Low, Medium, High)\n",
        "agg[\"demand_level\"] = pd.qcut(agg[\"ride_count\"], q=3, labels=[\"Low\", \"Medium\", \"High\"])\n",
        "\n",
        "# Encode categorical zone\n",
        "le = LabelEncoder()\n",
        "agg[\"zone_enc\"] = le.fit_transform(agg[zone_col])\n",
        "\n",
        "# Features and target\n",
        "X = agg[[\"zone_enc\", \"hour\", \"dayofweek\", \"is_weekend\", \"ride_count\"]]\n",
        "y = agg[\"demand_level\"]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ---------------------------------\n",
        "# STEP 2: Bayesian Optimization for XGBoost\n",
        "# ---------------------------------\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 500),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
        "        \"random_state\": 42,\n",
        "        \"eval_metric\": \"mlogloss\"\n",
        "    }\n",
        "    model = XGBClassifier(**params)\n",
        "    scores = cross_val_score(model, X_train_scaled, y_train, cv=3, scoring='accuracy')\n",
        "    return scores.mean()\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=25)\n",
        "best_xgb_params = study.best_params\n",
        "print(\"\\nâœ… Best XGBoost Params:\", best_xgb_params)\n",
        "\n",
        "# ---------------------------------\n",
        "# STEP 3: Train Optimized Base Models\n",
        "# ---------------------------------\n",
        "xgb_opt = XGBClassifier(**best_xgb_params)\n",
        "lgb_opt = LGBMClassifier(\n",
        "    n_estimators=350,\n",
        "    learning_rate=0.07,\n",
        "    max_depth=8,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "gb_opt = GradientBoostingClassifier(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.08,\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------------\n",
        "# STEP 4: Stacking (Hybrid Ensemble)\n",
        "# ---------------------------------\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_opt),\n",
        "        ('lgb', lgb_opt),\n",
        "        ('gb', gb_opt)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(max_iter=1000),\n",
        "    passthrough=True\n",
        ")\n",
        "\n",
        "# ---------------------------------\n",
        "# STEP 5: Train & Evaluate\n",
        "# ---------------------------------\n",
        "models = {\n",
        "    \"XGBoost (Tuned)\": xgb_opt,\n",
        "    \"LightGBM (Optimized)\": lgb_opt,\n",
        "    \"Gradient Boosting\": gb_opt,\n",
        "    \"Hybrid Optimized Ensemble (HOE)\": stacking_model\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    preds = model.predict(X_test_scaled)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    results[name] = acc\n",
        "\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(\"Accuracy:\", round(acc, 4))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, preds))\n",
        "    cm = confusion_matrix(y_test, preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f\"Confusion Matrix - {name}\")\n",
        "    plt.show()\n",
        "\n",
        "# ---------------------------------\n",
        "# STEP 6: Compare All Models\n",
        "# ---------------------------------\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=list(results.keys()), y=list(results.values()), palette=\"coolwarm\")\n",
        "plt.title(\"Model Accuracy Comparison (Optimized)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "best_model = max(results, key=results.get)\n",
        "print(f\"\\nðŸ† Best Model: {best_model} with Accuracy = {results[best_model]:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "bfGlgR6u8r35"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}